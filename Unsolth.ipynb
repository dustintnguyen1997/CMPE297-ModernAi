{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U \"unsloth[cpu,cu121]\" --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -U transformers==4.39.3\n",
        "!pip install -U datasets==2.19.1\n",
        "!pip install -U accelerate==0.28.0\n",
        "!pip install -U trl==0.7.11\n",
        "!pip install -U bitsandbytes==0.43.0\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer, SFTConfig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "y0er1ciYUmz8",
        "outputId": "f264277c-9919-4ebf-9c0b-aa34922193df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "numpy.core.multiarray failed to import",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-232192996.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSFTConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.19.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murl_to_fs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0m_gc_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misenabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0m_gc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_gc_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0m_gc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/lib.pyx\u001b[0m in \u001b[0;36minit pyarrow.lib\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################################### A ###########################################\n",
        "# Make sure you're on a GPU runtime:\n",
        "# Runtime -> Change runtime type -> Hardware accelerator: GPU\n",
        "\n",
        "import torch, platform, psutil\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"RAM (GB):\", round(psutil.virtual_memory().total / 1e9, 2))\n",
        "\n",
        "# Basic training config\n",
        "MODEL_NAME = \"unsloth/SmolLM2-135M-Instruct\"  # SmolLM2 135M instruct, Unsloth-optimized\n",
        "MAX_SEQ_LEN = 512\n",
        "DTYPE = None  # Let Unsloth choose (fp16/bf16 depending on GPU)\n",
        "\n",
        "OUTPUT_DIR = \"smollm2_135m_full_finetune_alpaca\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Using model:\", MODEL_NAME)\n",
        "\n",
        "# Load a subset of the Alpaca dataset\n",
        "raw_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:1000]\")  # first 1000 examples\n",
        "raw_dataset\n",
        "\n",
        "def format_example(example):\n",
        "    instruction = example[\"instruction\"]\n",
        "    inp = example.get(\"input\", \"\")\n",
        "    output = example[\"output\"]\n",
        "\n",
        "    if inp is None:\n",
        "        inp = \"\"\n",
        "\n",
        "    if inp.strip():\n",
        "        text = (\n",
        "            \"### Instruction:\\n\"\n",
        "            f\"{instruction}\\n\\n\"\n",
        "            \"### Input:\\n\"\n",
        "            f\"{inp}\\n\\n\"\n",
        "            \"### Response:\\n\"\n",
        "            f\"{output}\"\n",
        "        )\n",
        "    else:\n",
        "        text = (\n",
        "            \"### Instruction:\\n\"\n",
        "            f\"{instruction}\\n\\n\"\n",
        "            \"### Response:\\n\"\n",
        "            f\"{output}\"\n",
        "        )\n",
        "    return {\"text\": text}\n",
        "\n",
        "processed_dataset = raw_dataset.map(format_example, remove_columns=raw_dataset.column_names)\n",
        "processed_dataset = processed_dataset.shuffle(seed=42)\n",
        "processed_dataset[0]\n",
        "\n",
        "#Load SmolLM2-135M with full_finetuning=True\n",
        "max_seq_length = MAX_SEQ_LEN\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name      = MODEL_NAME,\n",
        "    max_seq_length  = max_seq_length,\n",
        "    dtype           = DTYPE,\n",
        "    load_in_4bit    = False,           # full 16-bit weights\n",
        "    full_finetuning = True,            # ‚úÖ FULL FINETUNING\n",
        ")\n",
        "\n",
        "# Put model into training mode (Unsloth patches + gradient config)\n",
        "FastLanguageModel.for_training(model)\n",
        "\n",
        "# Make sure tokenizer has a padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Pad token:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
        "\n",
        "train_config = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,      # effective batch size = 16\n",
        "    learning_rate=5e-5,                 # slightly conservative for full FT on small model\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    bf16=torch.cuda.is_available(),     # use bf16 if supported\n",
        "    fp16=not torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False,\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        ")\n",
        "train_config\n",
        "\n",
        "# Create trainer & start full fine-tuning\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=processed_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=True,           # pack multiple samples into one sequence to use context better\n",
        "    args=train_config,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "BBhdyV4GUJME",
        "outputId": "14cdb50d-0dba-46e1-f5e3-232cc18a79a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Python: 3.12.12\n",
            "RAM (GB): 54.75\n",
            "Using model: unsloth/SmolLM2-135M-Instruct\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1314534160.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load a subset of the Alpaca dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mraw_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tatsu-lab/alpaca\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train[:1000]\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# first 1000 examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mraw_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################################### B #######################\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Basic training config\n",
        "MODEL_NAME = \"unsloth/SmolLM2-135M-Instruct\"\n",
        "MAX_SEQ_LEN = 512\n",
        "DTYPE = None  # Let Unsloth choose (fp16/bf16)\n",
        "\n",
        "OUTPUT_DIR = \"smollm2_135m_lora_finetune_alpaca\"  # üëà different from full FT\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Using model:\", MODEL_NAME)\n",
        "\n",
        "def format_example(example):\n",
        "    instruction = example[\"instruction\"]\n",
        "    inp = example.get(\"input\", \"\") or \"\"\n",
        "    output = example[\"output\"]\n",
        "\n",
        "    if inp.strip():\n",
        "        text = (\n",
        "            \"### Instruction:\\n\"\n",
        "            f\"{instruction}\\n\\n\"\n",
        "            \"### Input:\\n\"\n",
        "            f\"{inp}\\n\\n\"\n",
        "            \"### Response:\\n\"\n",
        "            f\"{output}\"\n",
        "        )\n",
        "    else:\n",
        "        text = (\n",
        "            \"### Instruction:\\n\"\n",
        "            f\"{instruction}\\n\\n\"\n",
        "            \"### Response:\\n\"\n",
        "            f\"{output}\"\n",
        "        )\n",
        "    return {\"text\": text}\n",
        "\n",
        "processed_dataset = raw_dataset.map(\n",
        "    format_example,\n",
        "    remove_columns=raw_dataset.column_names,\n",
        ")\n",
        "processed_dataset = processed_dataset.shuffle(seed=42)\n",
        "processed_dataset[0]\n",
        "\n",
        "max_seq_length = MAX_SEQ_LEN\n",
        "\n",
        "# 1) Load base model in 4-bit for parameter-efficient training\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name      = MODEL_NAME,\n",
        "    max_seq_length  = max_seq_length,\n",
        "    dtype           = DTYPE,\n",
        "    load_in_4bit    = True,    # ‚úÖ 4-bit + LoRA\n",
        "    full_finetuning = False,   # ‚úÖ NOT full finetuning\n",
        ")\n",
        "\n",
        "# 2) Turn this base model into a LoRA model\n",
        "# You can tweak r, lora_alpha, lora_dropout if needed\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Prepare for training\n",
        "FastLanguageModel.for_training(model)\n",
        "\n",
        "# Padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Pad token:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
        "\n",
        "\n",
        "# slighty different finetuning\n",
        "train_config = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,               # üîº a bit higher LR for LoRA\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    bf16=torch.cuda.is_available(),\n",
        "    fp16=not torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False,\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        ")\n",
        "train_config\n",
        "\n",
        "#same as colab 1\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=processed_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=True,\n",
        "    args=train_config,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "#lora model\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"Saved LoRA adapter + tokenizer to:\", OUTPUT_DIR)\n",
        "\n",
        "#quick lora test\n",
        "from transformers import TextStreamer\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "def generate(prompt, max_new_tokens=128):\n",
        "    formatted = (\n",
        "        \"### Instruction:\\n\"\n",
        "        f\"{prompt}\\n\\n\"\n",
        "        \"### Response:\\n\"\n",
        "    )\n",
        "    inputs = tokenizer(\n",
        "        formatted,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LEN,\n",
        "    ).to(model.device)\n",
        "\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "    _ = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        streamer=streamer,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "test_prompt = \"Write a short, friendly email apologizing for a delayed response.\"\n",
        "generate(test_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "Dv_NJYPCUg3H",
        "outputId": "aeb6aef2-eb7e-4eae-f9a3-385e57fdccfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'BeamBasedBuilder' from 'datasets.builder' (/usr/local/lib/python3.12/dist-packages/datasets/builder.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-441284076.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSFTConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcombine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcatenate_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterleave_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset_dict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableDatasetDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'BeamBasedBuilder' from 'datasets.builder' (/usr/local/lib/python3.12/dist-packages/datasets/builder.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################## C #################################################\n",
        "\n",
        "OUTPUT_DIR = \"smollm2_135m_dpo_orca\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Using model:\", MODEL_NAME)\n",
        "\n",
        "#load dataset\n",
        "dataset_raw = load_dataset(\"Intel/orca_dpo_pairs\", split=\"train\")  # large; we'll slice it\n",
        "dataset_raw\n",
        "\n",
        "# small subset for demo (you can increase later)\n",
        "dataset_raw_small = dataset_raw.shuffle(seed=42).select(range(2000))\n",
        "len(dataset_raw_small), dataset_raw_small[0]\n",
        "\n",
        "# convert to prompt\n",
        "def make_preference(example):\n",
        "    prompt = example[\"question\"]\n",
        "    chosen = example[\"chosen\"]\n",
        "    rejected = example[\"rejected\"]\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": chosen,\n",
        "        \"rejected\": rejected,\n",
        "    }\n",
        "\n",
        "pref_dataset = dataset_raw_small.map(\n",
        "    make_preference,\n",
        "    remove_columns=dataset_raw_small.column_names,\n",
        ")\n",
        "pref_dataset[0]\n",
        "\n",
        "# different chat templates\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Load model + tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name     = MODEL_NAME,\n",
        "    max_seq_length = MAX_SEQ_LEN,\n",
        "    dtype          = DTYPE,\n",
        "    load_in_4bit   = LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "# Use a Zephyr-style chat template (good generic default)\n",
        "tokenizer.chat_template = get_chat_template(tokenizer, \"zephyr\")\n",
        "\n",
        "# Turn the base model into a LoRA model (PEFT)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_training(model)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Pad token:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
        "\n",
        "#training config\n",
        "dpo_config = DPOConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=1,          # you can increase later\n",
        "    learning_rate=5e-5,\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    warmup_ratio=0.05,\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    max_length=MAX_SEQ_LEN,\n",
        "    max_prompt_length=256,\n",
        ")\n",
        "\n",
        "dpo_config\n",
        "\n",
        "# creating DPOT training\n",
        "from trl import DPOTrainer\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model        = model,\n",
        "    ref_model    = None,          # if None, TRL clones a frozen reference model internally\n",
        "    args         = dpo_config,\n",
        "    train_dataset= pref_dataset,\n",
        "    eval_dataset = None,\n",
        "    tokenizer    = tokenizer,\n",
        "    max_length   = MAX_SEQ_LEN,\n",
        "    max_prompt_length = 256,\n",
        "    beta         = 0.1,           # strength of preference signal\n",
        ")\n",
        "\n",
        "dpo_trainer.train()\n"
      ],
      "metadata": {
        "id": "fgVfdcUtXEbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################# D ###########################################\n",
        "\n",
        "#Load reasoning model with Unsloth + LoRA\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "import torch\n",
        "\n",
        "max_seq_length = 1024      # can increase for longer reasoning traces\n",
        "lora_rank      = 32        # LoRA rank; higher = more capacity, more VRAM\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"google/gemma-3-1b-it\",   # you can swap to Qwen3 / Llama 3.2 1B, etc.\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,                   # 4-bit QLoRA\n",
        "    fast_inference=True,                 # enable vLLM-backed fast inference\n",
        "    max_lora_rank=lora_rank,\n",
        "    gpu_memory_utilization=0.6,          # lower if you OOM\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=lora_rank,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Loaded model:\", model.__class__.__name__)\n",
        "print(\"Pad token:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
        "\n",
        "# System prompt that enforces an XML ‚Äúreasoning + answer‚Äù structure\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a math reasoning assistant.\n",
        "\n",
        "Respond in the following exact format:\n",
        "<reasoning>\n",
        "...step by step reasoning here...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...final numeric answer ONLY here...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    # Pull the content inside <answer>...</answer>\n",
        "    answer = text.split(\"<answer>\")[-1]\n",
        "    answer = answer.split(\"</answer>\")[0]\n",
        "    return answer.strip()\n",
        "\n",
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    # GSM8K stores answer like \"‚Ä¶#### 42\"\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "def get_gsm8k_questions(split: str = \"train\") -> Dataset:\n",
        "    data = load_dataset(\"openai/gsm8k\", \"main\")[split]\n",
        "    # For speed, take a small subset for demo; increase for real runs\n",
        "    data = data.shuffle(seed=42).select(range(500))\n",
        "\n",
        "    data = data.map(\n",
        "        lambda x: {\n",
        "            \"prompt\": [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
        "            ],\n",
        "            \"answer\": extract_hash_answer(x[\"answer\"]),\n",
        "        }\n",
        "    )\n",
        "    return data\n",
        "\n",
        "dataset = get_gsm8k_questions(\"train\")\n",
        "dataset[0]\n",
        "\n",
        "# 1) Reward: correctness (does final answer match ground truth?)\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    # completions: list[list[{\"role\": \"assistant\", \"content\": \"...\"}]]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    q = prompts[0][-1][\"content\"]  # last user question for logging\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "\n",
        "    print(\n",
        "        \"-\" * 40,\n",
        "        f\"\\nQuestion:\\n{q}\",\n",
        "        f\"\\nGold answer:\\n{answer[0]}\",\n",
        "        f\"\\nModel full response:\\n{responses[0]}\",\n",
        "        f\"\\nExtracted final answer:\\n{extracted_responses[0]}\",\n",
        "    )\n",
        "\n",
        "    # reward 2.0 if exact numeric match, else 0\n",
        "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
        "\n",
        "\n",
        "# 2) Reward: is the final answer an integer?\n",
        "def int_reward_func(completions, **kwargs) -> list[float]:\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
        "\n",
        "\n",
        "# 3) Reward: strict XML format\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n?$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "\n",
        "# 4) Reward: soft XML format (less strict)\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.search(pattern, r, flags=re.DOTALL) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "\n",
        "# 5) Reward: XML tag counting + penalize junk after </answer>\n",
        "def count_xml(text: str) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        # penalize characters after </answer>\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "max_prompt_length = 256\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,  # you can set 4 for smoother updates\n",
        "    num_generations=6,              # number of samples per prompt\n",
        "    max_prompt_length=max_prompt_length,\n",
        "    max_completion_length=max_seq_length - max_prompt_length,\n",
        "    max_steps=250,                  # demo; for real runs use 300+ as docs suggest\n",
        "    save_steps=250,\n",
        "    max_grad_norm=0.1,\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"grpo_gemma3_1b_gsm8k\",\n",
        "    # Optional advanced GRPO variants (from Unsloth docs):\n",
        "    # loss_type=\"grpo\",\n",
        "    # epsilon=0.2,\n",
        "    # epsilon_high=0.28,\n",
        "    # delta=1.5,\n",
        "    # mask_truncated_completions=True,\n",
        ")\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[\n",
        "        xmlcount_reward_func,\n",
        "        soft_format_reward_func,\n",
        "        strict_format_reward_func,\n",
        "        int_reward_func,\n",
        "        correctness_reward_func,\n",
        "    ],\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "mGC4ZHQKXbwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################### E #############################################\n",
        "!nvidia-smi\n",
        "!pip install -q \"unsloth>=2024.10.0\" \"transformers>=4.45.0\" \\\n",
        "               \"accelerate>=1.0.0\" \"datasets>=3.0.0\" \\\n",
        "               \"bitsandbytes>=0.43.0\"\n",
        "\n",
        "from unsloth import FastLanguageModel, UnslothTrainer, UnslothTrainingArguments\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import torch, textwrap, os, random\n",
        "\n",
        "max_seq_length = 2048     # context length\n",
        "dtype = None              # let Unsloth pick bf16/float16 if available\n",
        "load_in_4bit = True       # QLoRA style\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "#Load the base model (before CPT)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name      = model_name,\n",
        "    max_seq_length  = max_seq_length,\n",
        "    dtype           = dtype,\n",
        "    load_in_4bit    = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Make sure padding is configured correctly\n",
        "tokenizer.pad_token    = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"Loaded model:\", model_name)\n",
        "\n",
        "\n",
        "#preparing new language\n",
        "import os\n",
        "\n",
        "corpus_path = \"new_language_corpus.txt\"\n",
        "\n",
        "if not os.path.exists(corpus_path):\n",
        "    # Tiny dummy corpus so the notebook still runs even without your real data.\n",
        "    # Replace this with a real language corpus (e.g. Tamil, Telugu, Finnish, etc.).\n",
        "    dummy_lines = [\n",
        "        \"mavo lira sento kala miro.\",\n",
        "        \"tora selen avo mira lavo.\",\n",
        "        \"kalo miro siven tala voru.\",\n",
        "        \"mira lira kavo sento luno.\",\n",
        "    ]\n",
        "    with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for line in dummy_lines:\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"{corpus_path} not found ‚Äì created a tiny dummy corpus.\")\n",
        "else:\n",
        "    print(f\"Found corpus file: {corpus_path}\")\n",
        "\n",
        "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = [l.strip() for l in f if l.strip()]\n",
        "\n",
        "print(f\"Loaded {len(lines)} lines from corpus.\")\n",
        "print(\"Example lines:\\n\")\n",
        "print(\"\\n\".join(lines[:5]))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_lines, val_lines = train_test_split(lines, test_size=0.05, random_state=seed)\n",
        "\n",
        "train_dataset = Dataset.from_dict({\"text\": train_lines})\n",
        "eval_dataset  = Dataset.from_dict({\"text\": val_lines})\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": eval_dataset,\n",
        "})\n",
        "\n",
        "dataset\n",
        "\n",
        "\n",
        "#before CPT sample\n",
        "FastLanguageModel.for_inference(model)  # switches to faster inference mode\n",
        "\n",
        "def generate(text, max_new_tokens=80):\n",
        "    prompt_ids = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=False,\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **prompt_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "test_prompt = lines[0] if len(lines) > 0 else \"mavo lira\"\n",
        "print(\"=== BEFORE CPT ===\")\n",
        "print(generate(test_prompt))\n",
        "\n",
        "#Add LoRA for continued pretraining\n",
        "# Switch back to train mode\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r              = 32,   # rank\n",
        "    lora_alpha     = 16,\n",
        "    lora_dropout   = 0.0,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        \"lm_head\", \"embed_tokens\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "#Data collator for language modeling\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer = tokenizer,\n",
        "    mlm       = False,  # causal LM, NOT masked LM\n",
        ")\n",
        "\n",
        "#define unsloth training arguments\n",
        "output_dir = \"llama32_new_language_cpt\"\n",
        "\n",
        "training_args = UnslothTrainingArguments(\n",
        "    output_dir                      = output_dir,\n",
        "    per_device_train_batch_size     = 2,\n",
        "    gradient_accumulation_steps     = 4,\n",
        "    max_steps                       = 200,   # for demo; use more for real training\n",
        "    warmup_steps                    = 20,\n",
        "    logging_steps                   = 10,\n",
        "    save_strategy                   = \"steps\",\n",
        "    save_steps                      = 100,\n",
        "    learning_rate                   = 5e-5,\n",
        "    embedding_learning_rate         = 5e-6,  # 10x smaller for lm_head/embed_tokens\n",
        "    bf16                            = torch.cuda.is_available(),\n",
        "    lr_scheduler_type              = \"cosine\",\n",
        "    weight_decay                    = 0.01,\n",
        ")\n",
        "\n",
        "#create and run CPT\n",
        "trainer = UnslothTrainer(\n",
        "    model         = model,\n",
        "    tokenizer     = tokenizer,\n",
        "    args          = training_args,\n",
        "    train_dataset = dataset[\"train\"],\n",
        "    eval_dataset  = dataset[\"validation\"],\n",
        "    data_collator = data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "7TexSWNrXvJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RaWzPNvcXPMm"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}